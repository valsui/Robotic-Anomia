import * as tf from '@tensorflow/tfjs';
import { inputTFData, outputTFData, aInput, aHotOnes } from './data';
import { MnistData } from './mnist';
// import { imageToMatrix } from './heatmap';
// source is following example for MNIST - https://js.tensorflow.org/tutorials/mnist.html
// ------- building the model structure and layers --------------
// simplest model in which tensors are consecutively passed from one layer to the next;
const model = tf.sequential();

// add first layer - two-dimensional convolutional layer. Convolutions slide a filter window over an image to learn transformations that are spatially invariant (that is, patterns or objects in different parts of the image will be treated the same way).
model.add(tf.layers.conv2d({
    inputShape: [25, 25, 1], //The shape of the data that will flow into the first layer of the model
    kernelSize: 5, //size of the sliding convolutional filter windows to be applied to the input data (5x5)
    filters: 8, //The number of filter windows of size
    strides: 1, //how many pixels the filter will shift 
    activation: 'relu',
    kernelInitializer: 'varianceScaling' // method used to randomize the model weights 
}));

//second layer - a max pooling layer. This layer will downsample the result (also known as the activation) from the convolution by computing the maximum value for each sliding window

model.add(tf.layers.maxPooling2d({
    poolSize: [2, 2], //The size of the sliding pooling windows to be applied to the input data
    strides: [2, 2] //step size of sliding window
}))

// additional layers in the network. Repeating layer structure is a common pattern in neural networks. Let's add a second convolutional layer, followed by another pooling layer to our model. Note that in our second convolutional layer, we're doubling the number of filters from 8 to 16. Input size is inferred from the first layer

model.add(tf.layers.conv2d({
    kernelSize: 5,
    filters: 16, 
    strides: 1,
    activation: 'relu',
    kernelInitializer: 'varianceScaling'
}));
// using categorical Crossentropy as a loss function which is commonly used to optimize classification tasks - it measures the error between the probability distribution generated by the last layer of our model and the probability distribution given by our label

model.add(tf.layers.maxPooling2d({
    poolSize: [2, 2],
    strides: [2, 2]
}));

//flatten output of previous layer
model.add(tf.layers.flatten());

// adding dense layer - fully connected layer that performs the classification 
//flattens the output of a convolution + pooling layer pair before dense layer - common pattern in NN

model.add(tf.layers.dense({
    units: 1, // size of output classification 
    kernelInitializer: 'varianceScaling',
    activation: 'softmax' //activation function of the last layer fro the classification task - usually softmax which normalizes output vector into a probability distribution
}));

//------- defining optimizer and loss function -------
const LEARNING_RATE = 0.3;
const optimizer = tf.train.sgd(LEARNING_RATE); //stochastic gradient descent 

//------- compile the model  ------------------
//evaluation metric is just the accuracy

model.compile({
    optimizer: optimizer,
    loss: 'categoricalCrossentropy',
    metrics: ['accuracy']
})

//-------- defining BATCH SIZES ---------
// How many examples the model should "see" before making a parameter update.
const BATCH_SIZE = 20;
// How many batches to train the model for.
const TRAIN_BATCHES = 10;

// Every TEST_ITERATION_FREQUENCY batches, test accuracy over TEST_BATCH_SIZE examples.
// Ideally, we'd compute accuracy over the whole test set, but for performance
// reasons we'll use a subset.
const TEST_BATCH_SIZE = 10;
const TEST_ITERATION_FREQUENCY = 1;

//When we set a BATCH_SIZE of 64, we're batching 64 images at a time, which means the actual shape of our data is [64, 28, 28, 1] (the batch is always the outermost dimension).

// NOTE:* Recall that the inputShape in the config of our first conv2d did not specify the batch size(64).Configs are written to be batch - size - agnostic, so that they are able to accept batches of arbitrary size.

//-------- TRAINING LOOP --------------
//need to define data class that contains the two public methods:
// (1) nextTrainBatch(batchSize)-returns a random batch of images and their labels from the training set
// (2) nextTestBatch(batchSize)-returns a batch of images and their labels from the test set
async function train(){
    const lossValues = [];
    const accuracyValues = [];

    for (let i = 0; i < TRAIN_BATCHES; i++) {
        const batch = data.nextTrainBatch(BATCH_SIZE);
        
        let testBatch;
        let validationData;
        // Every few batches test the accuracy of the mode.
        if (i % TEST_ITERATION_FREQUENCY === 0) {
            testBatch = data.nextTestBatch(TEST_BATCH_SIZE);
            validationData = [
                testBatch.xs.reshape([TEST_BATCH_SIZE, 25, 25, 1]), testBatch.labels
            ];
        }
        
        // The entire dataset doesn't fit into memory so we call fit repeatedly
        // with batches.
        const history = await model.fit(
            batch.xs.reshape([BATCH_SIZE, 25, 25, 1]),
            batch.labels,
            {
                batchSize: BATCH_SIZE,
                validationData,
                epochs: 1
            });
            
            const loss = history.history.loss[0];
            const accuracy = history.history.acc[0];
            
            // ... plotting code ...
        }
    }
async function showPredictions(){
    const testExamples = 50;
    const batch = data.nextTestBatch(testExamples);
    tf.tidy( () => {
        const output = model.predict(batch.xs.reshape([-1, 25, 25, 1]));
        console.log('output:',output);
        const axis = 1; 
        const labels = Array.from(batch.labels.argMax(axis).dataSync());
        const predictions = Array.from(output.argMax(axis).dataSync());
        // console.log(predictions);
        console.log(labels);
    }) 
}
let data;
async function load(){
    data = new MnistData();
    await data.load();
}
        
async function mnist(){
    await load();
    await train();
    showPredictions();
}

mnist();

export default model;